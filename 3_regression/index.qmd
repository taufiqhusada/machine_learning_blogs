---
title: Linear and Non-linear Regression
jupyter: python3
---

In here we will try to use linear and non-linear regression to predict car price


```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
!ls data
```

```{python}
df = pd.read_csv('data/CarPrice_Assignment.csv')
```

```{python}
df
```

```{python}
df.info()
```

```{python}
#| scrolled: false
# Create a histogram to visualize the distribution of car prices
plt.figure(figsize=(8, 6))
plt.hist(df['price'], bins=20, color='skyblue', edgecolor='black')
plt.title('Car Price Distribution')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()
```

```{python}
correlation_matrix = df.corr()

# Visualize the correlation matrix as a heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()
```

Based on the heatmap above we can see that numerical features that highly correlates with price are 'wheelbase', 'carlength', 'carwidth', 'curbweight', 'enginesize', 'boreratio','horsepower', 'citympg', 'highwaympg', 'price'

```{python}
# get the company name
CompanyName = df['CarName'].apply(lambda x : x.split(' ')[0])
df.insert(3,"CompanyName",CompanyName)
df.drop(['CarName'],axis=1,inplace=True)
df.head()
```

```{python}
plt.figure(figsize=(25, 6))

temp = pd.DataFrame(df.groupby(['CompanyName'])['price'].mean().sort_values(ascending = False))
temp.plot.bar()
plt.title('Company Name vs Average Price')
plt.show()
```

Based on the histogram above we can see that company name really affects the price of the car

```{python}
selected_columns = ['CompanyName','wheelbase', 'carlength', 'carwidth', 'curbweight', 'enginesize', 'boreratio',
                    'horsepower', 'citympg', 'highwaympg', 'price']
```

```{python}
# Selecting relevant columns for the model
df = df[selected_columns]

# Convert categorical variables to numerical using one-hot encoding if needed
df = pd.get_dummies(df)

# Splitting the data into features (X) and target variable (y)
X = df.drop('price', axis=1)
y = df['price']

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## Linear Regression

```{python}
# Linear Regression
linear_model = LinearRegression()

# Training the model
linear_model.fit(X_train, y_train)

# Making predictions
y_pred_linear = linear_model.predict(X_test)

# Calculate accuracy metrics
mse_linear = mean_squared_error(y_test, y_pred_linear)
r2_linear = r2_score(y_test, y_pred_linear)

# Visualizing the results (Predicted vs. Actual)
plt.scatter(y_test, y_pred_linear)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Linear Regression: Actual vs. Predicted Price")
plt.show()

print(f"Linear Regression Model - Mean Squared Error: {mse_linear}")
print(f"Linear Regression Model - R-squared Score: {r2_linear}")
```

## Non-Linear Regression (Random Forest Regressor)

```{python}
# Random Forest Regression
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Training the model
random_forest_model.fit(X_train, y_train)

# Making predictions
y_pred_rf = random_forest_model.predict(X_test)

# Calculate accuracy metrics
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

# Visualizing the results (Predicted vs. Actual)
plt.scatter(y_test, y_pred_rf)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Random Forest Regression: Actual vs. Predicted Price")
plt.show()

print(f"Random Forest Regression Model - Mean Squared Error: {mse_rf}")
print(f"Random Forest Regression Model - R-squared Score: {r2_rf}")
```


