{"title":"Learning Classification Algorithm using Titanic Dataset","markdown":{"yaml":{"title":"Learning Classification Algorithm using Titanic Dataset","jupyter":"python3"},"headingText":"Data Exploration","containsRefs":false,"markdown":"\n\nIn this blog we will learn some classfication algorithm using the famous titanic dataset\n<ul>\n<li> dataset: https://www.kaggle.com/competitions/titanic/data </li>\n<li> task: build a predictive model that predict whether a person survive in the titanic insident based on several factors (e.g., name, age, cabin, etc.) </li>\n</ul>\nWe will learn some basic data exploration technique, feature engineering, and classification algorithm\n\n```{python}\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n\nLets see the preview of the dataset\n\n```{python}\ndf_train = pd.read_csv('data/train.csv')\ndf_train.head()\n```\n\n```{python}\ndf_test = pd.read_csv('data/test.csv')\ndf_test.head()\n```\n\nNow let's see some pattern on the dataset\n\n```{python}\nsns.barplot(x=df_train['Sex'], y=df_train['Survived']) \n```\n\nBased on the plot above we can see that female are much more likely to survive compared to men\n\nNow let's find the correlation among the features on the data using heatmap\n\n```{python}\nsns.heatmap(df_train.corr(),annot=True) \nfig=plt.gcf()\nplt.show()\n```\n\nBased on the heatmap above we can see that \"PClass\" and \"Fare\" is highly correlated to the \"Survived\" column, whereas other column is not highly correlated with \"Survived\" column. <br>\nBut since \"Fare\" also highly correlated with \"PClass\" then we just need \"PClass\" column\n\n## Feature Engineering\n\nBased on our data exploration above we will only use column \"Sex\" and \"PClass\" to predict the survival of each person since those two features are highly correlated with the survival\n\nSo here is the step of feature engineering that we will do:\n\n1. Convert \"Sex\" column into a 0/1 valued column since this column has a categorical data (\"female/male\"). We can call this column \"IsMale\"\n2. Create new column TicketFreq based on column Ticket. Will explain about this later\n3. Drop columns other unnecessary column\n\n```{python}\n## Create new column \"IsMale\"\n\ndf_train_transformed = df_train.copy()\ndf_train_transformed[\"IsMale\"] = df_train[\"Sex\"].apply(lambda x: x == \"male\")\ndf_train_transformed.head()\n```\n\nTicket Frequency is a ticket-based feature that includes people who have the same ticket number. This feature can serve as group size as it puts people who travel with the same ticket number together, whether they are related or not.\n\n```{python}\ndf_train_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n```\n\n```{python}\n## Drop unwanted columns\ndf_train_transformed = df_train_transformed.drop(['PassengerId','Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \ndf_train_transformed.head()\n```\n\nSince we need to do the same feature engineering process on the test dataset, so let's wrap all the process into a single function\n\n```{python}\ndef transform_data(df, is_train_dataset = True):\n    df_transformed = df.copy()\n    df_transformed[\"IsMale\"] = df[\"Sex\"].apply(lambda x: x == \"male\")\n    \n    df_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n    \n    df_transformed = df_transformed.drop(['Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \n    \n    if (is_train_dataset):\n        df_transformed = df_transformed.drop(['PassengerId'],axis=1)\n    \n    return df_transformed\n```\n\n```{python}\ntemp_df = transform_data(df_train)\ntemp_df.head()\n```\n\n```{python}\nsns.barplot(x='IsMale', y='Survived', data=temp_df) \n```\n\n```{python}\nsns.barplot(x='TicketFreq', y='Survived', data=temp_df) \n```\n\n```{python}\nsns.barplot(x='Pclass', y='Survived', data=temp_df) \n```\n\n## Model Training\nIn this part we will experiment with several classifier model\n\nTo measure performance we will split the training data into `train` and `test`. So we will not touch the real `test` dataset to measure performance during our training process, this is to prevent data test leak. We will use 80/20 train-test split\n\n```{python}\npredictors = ['IsMale', 'Pclass', 'TicketFreq']\nlabel = 'Survived'\n```\n\n```{python}\ndf_train_transformed = transform_data(df_train)\n\nX = df_train_transformed[predictors]\ny = df_train_transformed[label]\n```\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n```\n\n```{python}\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n```\n\n```{python}\nfrom sklearn import metrics\n\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n```\n\n```{python}\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\n```\n\n```{python}\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n```\n\nBased on the comparison above, random forest classifier model gives better accuracy, that is why we will choose this model for our final prediction\n\n```{python}\ndf_test_transformed = transform_data(df_test)\nX_test_submission = df_test_transformed[predictors]\n```\n\n```{python}\nmodel = RandomForestClassifier()\n\nmodel.fit(X, y)\npredictions = model.predict(X_test_submission)\n```\n\n```{python}\noutput= pd.DataFrame (pd.DataFrame({\n    \"PassengerId\": df_test[\"PassengerId\"],\n    \"Survived\": predictions}))\noutput.head()\n```\n\n```{python}\noutput.to_csv('FinalSubmission.csv', index=False)\n```\n\nHere is the result when we submit this to kaggle competition\n![image](pict/Screenshot_result.png)\n\n","srcMarkdownNoYaml":"\n\nIn this blog we will learn some classfication algorithm using the famous titanic dataset\n<ul>\n<li> dataset: https://www.kaggle.com/competitions/titanic/data </li>\n<li> task: build a predictive model that predict whether a person survive in the titanic insident based on several factors (e.g., name, age, cabin, etc.) </li>\n</ul>\nWe will learn some basic data exploration technique, feature engineering, and classification algorithm\n\n```{python}\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n## Data Exploration\n\nLets see the preview of the dataset\n\n```{python}\ndf_train = pd.read_csv('data/train.csv')\ndf_train.head()\n```\n\n```{python}\ndf_test = pd.read_csv('data/test.csv')\ndf_test.head()\n```\n\nNow let's see some pattern on the dataset\n\n```{python}\nsns.barplot(x=df_train['Sex'], y=df_train['Survived']) \n```\n\nBased on the plot above we can see that female are much more likely to survive compared to men\n\nNow let's find the correlation among the features on the data using heatmap\n\n```{python}\nsns.heatmap(df_train.corr(),annot=True) \nfig=plt.gcf()\nplt.show()\n```\n\nBased on the heatmap above we can see that \"PClass\" and \"Fare\" is highly correlated to the \"Survived\" column, whereas other column is not highly correlated with \"Survived\" column. <br>\nBut since \"Fare\" also highly correlated with \"PClass\" then we just need \"PClass\" column\n\n## Feature Engineering\n\nBased on our data exploration above we will only use column \"Sex\" and \"PClass\" to predict the survival of each person since those two features are highly correlated with the survival\n\nSo here is the step of feature engineering that we will do:\n\n1. Convert \"Sex\" column into a 0/1 valued column since this column has a categorical data (\"female/male\"). We can call this column \"IsMale\"\n2. Create new column TicketFreq based on column Ticket. Will explain about this later\n3. Drop columns other unnecessary column\n\n```{python}\n## Create new column \"IsMale\"\n\ndf_train_transformed = df_train.copy()\ndf_train_transformed[\"IsMale\"] = df_train[\"Sex\"].apply(lambda x: x == \"male\")\ndf_train_transformed.head()\n```\n\nTicket Frequency is a ticket-based feature that includes people who have the same ticket number. This feature can serve as group size as it puts people who travel with the same ticket number together, whether they are related or not.\n\n```{python}\ndf_train_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n```\n\n```{python}\n## Drop unwanted columns\ndf_train_transformed = df_train_transformed.drop(['PassengerId','Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \ndf_train_transformed.head()\n```\n\nSince we need to do the same feature engineering process on the test dataset, so let's wrap all the process into a single function\n\n```{python}\ndef transform_data(df, is_train_dataset = True):\n    df_transformed = df.copy()\n    df_transformed[\"IsMale\"] = df[\"Sex\"].apply(lambda x: x == \"male\")\n    \n    df_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n    \n    df_transformed = df_transformed.drop(['Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \n    \n    if (is_train_dataset):\n        df_transformed = df_transformed.drop(['PassengerId'],axis=1)\n    \n    return df_transformed\n```\n\n```{python}\ntemp_df = transform_data(df_train)\ntemp_df.head()\n```\n\n```{python}\nsns.barplot(x='IsMale', y='Survived', data=temp_df) \n```\n\n```{python}\nsns.barplot(x='TicketFreq', y='Survived', data=temp_df) \n```\n\n```{python}\nsns.barplot(x='Pclass', y='Survived', data=temp_df) \n```\n\n## Model Training\nIn this part we will experiment with several classifier model\n\nTo measure performance we will split the training data into `train` and `test`. So we will not touch the real `test` dataset to measure performance during our training process, this is to prevent data test leak. We will use 80/20 train-test split\n\n```{python}\npredictors = ['IsMale', 'Pclass', 'TicketFreq']\nlabel = 'Survived'\n```\n\n```{python}\ndf_train_transformed = transform_data(df_train)\n\nX = df_train_transformed[predictors]\ny = df_train_transformed[label]\n```\n\n```{python}\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n```\n\n```{python}\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n```\n\n```{python}\nfrom sklearn import metrics\n\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n```\n\n```{python}\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\n```\n\n```{python}\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n```\n\nBased on the comparison above, random forest classifier model gives better accuracy, that is why we will choose this model for our final prediction\n\n```{python}\ndf_test_transformed = transform_data(df_test)\nX_test_submission = df_test_transformed[predictors]\n```\n\n```{python}\nmodel = RandomForestClassifier()\n\nmodel.fit(X, y)\npredictions = model.predict(X_test_submission)\n```\n\n```{python}\noutput= pd.DataFrame (pd.DataFrame({\n    \"PassengerId\": df_test[\"PassengerId\"],\n    \"Survived\": predictions}))\noutput.head()\n```\n\n```{python}\noutput.to_csv('FinalSubmission.csv', index=False)\n```\n\nHere is the result when we submit this to kaggle competition\n![image](pict/Screenshot_result.png)\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Learning Classification Algorithm using Titanic Dataset","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}