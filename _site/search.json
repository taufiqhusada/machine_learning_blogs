[
  {
    "objectID": "1_probability/index.html",
    "href": "1_probability/index.html",
    "title": "Random Variable And Probability",
    "section": "",
    "text": "We will explore some type of random variable.\nThen we will also use some probability techiniques (naive bayes) to do some prediction. We are gonna use customer behaviour dataset\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data/Customer_Behaviour.csv')\ndf\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15624510\nMale\n19\n19000\n0\n\n\n1\n15810944\nMale\n35\n20000\n0\n\n\n2\n15668575\nFemale\n26\n43000\n0\n\n\n3\n15603246\nFemale\n27\n57000\n0\n\n\n4\n15804002\nMale\n19\n76000\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n395\n15691863\nFemale\n46\n41000\n1\n\n\n396\n15706071\nMale\n51\n23000\n1\n\n\n397\n15654296\nFemale\n50\n20000\n1\n\n\n398\n15755018\nMale\n36\n33000\n0\n\n\n399\n15594041\nFemale\n49\n36000\n1\n\n\n\n\n400 rows × 5 columns\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 400 entries, 0 to 399\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   User ID          400 non-null    int64 \n 1   Gender           400 non-null    object\n 2   Age              400 non-null    int64 \n 3   EstimatedSalary  400 non-null    int64 \n 4   Purchased        400 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 15.8+ KB"
  },
  {
    "objectID": "1_probability/index.html#random-variable",
    "href": "1_probability/index.html#random-variable",
    "title": "Random Variable And Probability",
    "section": "Random Variable",
    "text": "Random Variable\n\ndf\n\n\n\n\n\n\n\n\nUser ID\nGender\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n15624510\nMale\n19\n19000\n0\n\n\n1\n15810944\nMale\n35\n20000\n0\n\n\n2\n15668575\nFemale\n26\n43000\n0\n\n\n3\n15603246\nFemale\n27\n57000\n0\n\n\n4\n15804002\nMale\n19\n76000\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n395\n15691863\nFemale\n46\n41000\n1\n\n\n396\n15706071\nMale\n51\n23000\n1\n\n\n397\n15654296\nFemale\n50\n20000\n1\n\n\n398\n15755018\nMale\n36\n33000\n0\n\n\n399\n15594041\nFemale\n49\n36000\n1\n\n\n\n\n400 rows × 5 columns\n\n\n\n\nContinuous Random Variable\n\n# Plot histograms for each numeric feature\nnumeric_features = df[['Age','EstimatedSalary']]\nnumeric_features.hist(figsize=(10, 8), bins=20, grid=False)\nplt.suptitle('Histograms of Numeric Features')\nplt.show()\n\n\n\n\n\n\nDiscrete Random Variable\n\n# Plot histogram for the 'Gender' feature\nplt.figure(figsize=(6, 4))\ndf['Gender'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'], edgecolor='black')\nplt.title('Histogram of Gender')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.show()"
  },
  {
    "objectID": "1_probability/index.html#naive-bayes-algorithm",
    "href": "1_probability/index.html#naive-bayes-algorithm",
    "title": "Random Variable And Probability",
    "section": "Naive Bayes Algorithm",
    "text": "Naive Bayes Algorithm\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Assuming your DataFrame is named 'df'\n# If not, replace 'df' with the actual name of your DataFrame\n\n# Selecting features and target variable\nX = df[['Gender', 'Age', 'EstimatedSalary']]\ny = df['Purchased']\n\n# Convert categorical variable 'Gender' into numerical values using one-hot encoding\nX = pd.get_dummies(X, columns=['Gender'], drop_first=True)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Create a Gaussian Naive Bayes classifier\nmodel = GaussianNB()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the performance of the model\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Confusion Matrix:\\n{conf_matrix}')\nprint(f'Classification Report:\\n{classification_rep}')\n\nAccuracy: 0.94\nConfusion Matrix:\n[[50  2]\n [ 3 25]]\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.94      0.96      0.95        52\n           1       0.93      0.89      0.91        28\n\n    accuracy                           0.94        80\n   macro avg       0.93      0.93      0.93        80\nweighted avg       0.94      0.94      0.94        80"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Author: Taufiq Husada Daryanto\nCourse: Machine Learning CS 5805\n\n\nList of blogs:\n\n\nProbability theory and random variables: Exploring some types of random variable and uses some probability theory techiniques (naive bayes) to do some prediction\nClustering: Clustering consumer based on consumer income and spending\nLinear and nonlinear regression: Regression to predict car price\nClassification: End-to-end machine learning projects using classification algorithm on titanic dataset\nAnomaly/outlier detection: Detect outlier in insurance charge prediction dataset then do some data cleaning and train regression model to predict insurance charges"
  },
  {
    "objectID": "3_regression/index.html",
    "href": "3_regression/index.html",
    "title": "Linear and Non-linear Regression",
    "section": "",
    "text": "In here we will try to use linear and non-linear regression to predict car price\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n!ls data\n\nCarPrice_Assignment.csv\ndf = pd.read_csv('data/CarPrice_Assignment.csv')\ndf\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n200\n201\n-1\nvolvo 145e (sw)\ngas\nstd\nfour\nsedan\nrwd\nfront\n109.1\n...\n141\nmpfi\n3.78\n3.15\n9.5\n114\n5400\n23\n28\n16845.0\n\n\n201\n202\n-1\nvolvo 144ea\ngas\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n...\n141\nmpfi\n3.78\n3.15\n8.7\n160\n5300\n19\n25\n19045.0\n\n\n202\n203\n-1\nvolvo 244dl\ngas\nstd\nfour\nsedan\nrwd\nfront\n109.1\n...\n173\nmpfi\n3.58\n2.87\n8.8\n134\n5500\n18\n23\n21485.0\n\n\n203\n204\n-1\nvolvo 246\ndiesel\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n...\n145\nidi\n3.01\n3.40\n23.0\n106\n4800\n26\n27\n22470.0\n\n\n204\n205\n-1\nvolvo 264gl\ngas\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n...\n141\nmpfi\n3.78\n3.15\n9.5\n114\n5400\n19\n25\n22625.0\n\n\n\n\n205 rows × 26 columns\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 205 entries, 0 to 204\nData columns (total 26 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   car_ID            205 non-null    int64  \n 1   symboling         205 non-null    int64  \n 2   CarName           205 non-null    object \n 3   fueltype          205 non-null    object \n 4   aspiration        205 non-null    object \n 5   doornumber        205 non-null    object \n 6   carbody           205 non-null    object \n 7   drivewheel        205 non-null    object \n 8   enginelocation    205 non-null    object \n 9   wheelbase         205 non-null    float64\n 10  carlength         205 non-null    float64\n 11  carwidth          205 non-null    float64\n 12  carheight         205 non-null    float64\n 13  curbweight        205 non-null    int64  \n 14  enginetype        205 non-null    object \n 15  cylindernumber    205 non-null    object \n 16  enginesize        205 non-null    int64  \n 17  fuelsystem        205 non-null    object \n 18  boreratio         205 non-null    float64\n 19  stroke            205 non-null    float64\n 20  compressionratio  205 non-null    float64\n 21  horsepower        205 non-null    int64  \n 22  peakrpm           205 non-null    int64  \n 23  citympg           205 non-null    int64  \n 24  highwaympg        205 non-null    int64  \n 25  price             205 non-null    float64\ndtypes: float64(8), int64(8), object(10)\nmemory usage: 41.8+ KB\n# Create a histogram to visualize the distribution of car prices\nplt.figure(figsize=(8, 6))\nplt.hist(df['price'], bins=20, color='skyblue', edgecolor='black')\nplt.title('Car Price Distribution')\nplt.xlabel('Price')\nplt.ylabel('Frequency')\nplt.show()\ncorrelation_matrix = df.corr()\n\n# Visualize the correlation matrix as a heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Matrix')\nplt.show()\n\n/var/folders/8l/h7ql3b751ld1q6km2kxd7kp00000gn/T/ipykernel_40100/3877527219.py:1: FutureWarning:\n\nThe default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\nBased on the heatmap above we can see that numerical features that highly correlates with price are ‘wheelbase’, ‘carlength’, ‘carwidth’, ‘curbweight’, ‘enginesize’, ‘boreratio’,‘horsepower’, ‘citympg’, ‘highwaympg’, ‘price’\n# get the company name\nCompanyName = df['CarName'].apply(lambda x : x.split(' ')[0])\ndf.insert(3,\"CompanyName\",CompanyName)\ndf.drop(['CarName'],axis=1,inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\ncar_ID\nsymboling\nCompanyName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\n\n\n0\n1\n3\nalfa-romero\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n1\n2\n3\nalfa-romero\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n2\n3\n1\nalfa-romero\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n3\n4\n2\naudi\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n4\n5\n2\naudi\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n5 rows × 26 columns\nplt.figure(figsize=(25, 6))\n\ntemp = pd.DataFrame(df.groupby(['CompanyName'])['price'].mean().sort_values(ascending = False))\ntemp.plot.bar()\nplt.title('Company Name vs Average Price')\nplt.show()\n\n&lt;Figure size 2400x576 with 0 Axes&gt;\nBased on the histogram above we can see that company name really affects the price of the car\nselected_columns = ['CompanyName','wheelbase', 'carlength', 'carwidth', 'curbweight', 'enginesize', 'boreratio',\n                    'horsepower', 'citympg', 'highwaympg', 'price']\n# Selecting relevant columns for the model\ndf = df[selected_columns]\n\n# Convert categorical variables to numerical using one-hot encoding if needed\ndf = pd.get_dummies(df)\n\n# Splitting the data into features (X) and target variable (y)\nX = df.drop('price', axis=1)\ny = df['price']\n\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "3_regression/index.html#linear-regression",
    "href": "3_regression/index.html#linear-regression",
    "title": "Linear and Non-linear Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\n\n# Linear Regression\nlinear_model = LinearRegression()\n\n# Training the model\nlinear_model.fit(X_train, y_train)\n\n# Making predictions\ny_pred_linear = linear_model.predict(X_test)\n\n# Calculate accuracy metrics\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\n\n# Visualizing the results (Predicted vs. Actual)\nplt.scatter(y_test, y_pred_linear)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Linear Regression: Actual vs. Predicted Price\")\nplt.show()\n\nprint(f\"Linear Regression Model - Mean Squared Error: {mse_linear}\")\nprint(f\"Linear Regression Model - R-squared Score: {r2_linear}\")\n\n\n\n\nLinear Regression Model - Mean Squared Error: 9285046.502511889\nLinear Regression Model - R-squared Score: 0.8823844238100562"
  },
  {
    "objectID": "3_regression/index.html#non-linear-regression-random-forest-regressor",
    "href": "3_regression/index.html#non-linear-regression-random-forest-regressor",
    "title": "Linear and Non-linear Regression",
    "section": "Non-Linear Regression (Random Forest Regressor)",
    "text": "Non-Linear Regression (Random Forest Regressor)\n\n# Random Forest Regression\nrandom_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Training the model\nrandom_forest_model.fit(X_train, y_train)\n\n# Making predictions\ny_pred_rf = random_forest_model.predict(X_test)\n\n# Calculate accuracy metrics\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\n\n# Visualizing the results (Predicted vs. Actual)\nplt.scatter(y_test, y_pred_rf)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Random Forest Regression: Actual vs. Predicted Price\")\nplt.show()\n\nprint(f\"Random Forest Regression Model - Mean Squared Error: {mse_rf}\")\nprint(f\"Random Forest Regression Model - R-squared Score: {r2_rf}\")\n\n\n\n\nRandom Forest Regression Model - Mean Squared Error: 3473973.0901777875\nRandom Forest Regression Model - R-squared Score: 0.9559944749270687"
  },
  {
    "objectID": "2_clustering/index.html",
    "href": "2_clustering/index.html",
    "title": "Clustering using Mall Customers Dataset",
    "section": "",
    "text": "In here we will learn about clustering using K-means algorithm\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data/Mall_Customers.csv')\ndf\n\n\n\n\n\n\n\n\nCustomerID\nGender\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n1\nMale\n19\n15\n39\n\n\n1\n2\nMale\n21\n15\n81\n\n\n2\n3\nFemale\n20\n16\n6\n\n\n3\n4\nFemale\n23\n16\n77\n\n\n4\n5\nFemale\n31\n17\n40\n\n\n...\n...\n...\n...\n...\n...\n\n\n195\n196\nFemale\n35\n120\n79\n\n\n196\n197\nFemale\n45\n126\n28\n\n\n197\n198\nMale\n32\n126\n74\n\n\n198\n199\nMale\n32\n137\n18\n\n\n199\n200\nMale\n30\n137\n83\n\n\n\n\n200 rows × 5 columns"
  },
  {
    "objectID": "2_clustering/index.html#dataset-explanation",
    "href": "2_clustering/index.html#dataset-explanation",
    "title": "Clustering using Mall Customers Dataset",
    "section": "Dataset explanation",
    "text": "Dataset explanation\n\nCustomerID: Unique identification number for each customer.\nGender: Categorical variable indicating the gender of the customer.\nAge: The age of the customer represented as an integer.\nAnnual Income (k$): Represents the annual income of the customer in thousands of dollars.\nSpending Score (1-100): An index or score (ranging from 1 to 100) that measures a customer’s spending behavior.\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 200 entries, 0 to 199\nData columns (total 5 columns):\n #   Column                  Non-Null Count  Dtype \n---  ------                  --------------  ----- \n 0   CustomerID              200 non-null    int64 \n 1   Gender                  200 non-null    object\n 2   Age                     200 non-null    int64 \n 3   Annual Income (k$)      200 non-null    int64 \n 4   Spending Score (1-100)  200 non-null    int64 \ndtypes: int64(4), object(1)\nmemory usage: 7.9+ KB\n\n\n\n# Selecting the features for clustering\nfeatures = df.iloc[:, 2:]  # Selecting columns from 'Age' onward\n\n\nfeatures\n\n\n\n\n\n\n\n\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\n0\n19\n15\n39\n\n\n1\n21\n15\n81\n\n\n2\n20\n16\n6\n\n\n3\n23\n16\n77\n\n\n4\n31\n17\n40\n\n\n...\n...\n...\n...\n\n\n195\n35\n120\n79\n\n\n196\n45\n126\n28\n\n\n197\n32\n126\n74\n\n\n198\n32\n137\n18\n\n\n199\n30\n137\n83\n\n\n\n\n200 rows × 3 columns"
  },
  {
    "objectID": "2_clustering/index.html#k-means",
    "href": "2_clustering/index.html#k-means",
    "title": "Clustering using Mall Customers Dataset",
    "section": "K-means",
    "text": "K-means\n\n# Using the Elbow method to find the optimal number of clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n    kmeans.fit(features)\n    wcss.append(kmeans.inertia_)\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n# Plotting the Elbow method graph to determine the optimal number of clusters\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, 11), wcss, marker='o', linestyle='--')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')  # Within cluster sum of squares\nplt.show()\n\n\n\n\n\n# Based on the elbow method, select the optimal number of clusters\noptimal_num_clusters = 5\n\n\n# Apply KMeans with the selected number of clusters\nkmeans = KMeans(n_clusters=optimal_num_clusters, init='k-means++', random_state=42)\nkmeans.fit(features)\n\n/Users/taufiq/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKMeans(n_clusters=5, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=5, random_state=42)\n\n\n\n# Add a column in the original DataFrame to show the cluster each data point belongs to\ndf['Cluster'] = kmeans.labels_\n\n\n# Visualize the clusters (considering 'Annual Income (k$)' and 'Spending Score')\nplt.figure(figsize=(8, 6))\nplt.scatter(df['Annual Income (k$)'], df['Spending Score (1-100)'], c=df['Cluster'], cmap='viridis')\nplt.title('KMeans Clustering')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score (1-100)')\nplt.show()"
  },
  {
    "objectID": "5_outlier/index.html",
    "href": "5_outlier/index.html",
    "title": "Outlier detection",
    "section": "",
    "text": "We will detect outlier in insurance charge prediction dataset then do some data cleaning and train regression model to predict insurance charges\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport matplotlib.pyplot as plt\ndf = pd.read_csv('data/insurance.csv')\ndf\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1333\n50\nmale\n30.970\n3\nno\nnorthwest\n10600.54830\n\n\n1334\n18\nfemale\n31.920\n0\nno\nnortheast\n2205.98080\n\n\n1335\n18\nfemale\n36.850\n0\nno\nsoutheast\n1629.83350\n\n\n1336\n21\nfemale\n25.800\n0\nno\nsouthwest\n2007.94500\n\n\n1337\n61\nfemale\n29.070\n0\nyes\nnorthwest\n29141.36030\n\n\n\n\n1338 rows × 7 columns\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1338 entries, 0 to 1337\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       1338 non-null   int64  \n 1   sex       1338 non-null   object \n 2   bmi       1338 non-null   float64\n 3   children  1338 non-null   int64  \n 4   smoker    1338 non-null   object \n 5   region    1338 non-null   object \n 6   charges   1338 non-null   float64\ndtypes: float64(2), int64(2), object(3)\nmemory usage: 73.3+ KB\n# Plotting a histogram of the 'charges' column\nplt.figure(figsize=(8, 6))\nplt.hist(df['charges'], bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribution of Charges')\nplt.xlabel('Charges')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "5_outlier/index.html#outlier-detection-using-iqr-method",
    "href": "5_outlier/index.html#outlier-detection-using-iqr-method",
    "title": "Outlier detection",
    "section": "Outlier Detection using IQR method",
    "text": "Outlier Detection using IQR method\n\n# Calculate the Interquartile Range (IQR)\nQ1 = df['charges'].quantile(0.25)\nQ3 = df['charges'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define the boundaries for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Find outliers\noutliers_iqr = df[(df['charges'] &lt; lower_bound) | (df['charges'] &gt; upper_bound)]\n\n# Plotting the boxplot for charges\nplt.figure(figsize=(8, 6))\nplt.boxplot(df['charges'], vert=False)\nplt.title('Boxplot of Charges with IQR Outliers')\nplt.xlabel('Charges')\nplt.yticks([1], ['Charges'])\nplt.scatter(outliers_iqr['charges'], [1] * len(outliers_iqr), color='red', label='Outliers')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\nprint(\"Outliers using IQR method:\")\nprint(outliers_iqr)\n\nOutliers using IQR method:\n      age     sex     bmi  children smoker     region      charges\n14     27    male  42.130         0    yes  southeast  39611.75770\n19     30    male  35.300         0    yes  southwest  36837.46700\n23     34  female  31.920         1    yes  northeast  37701.87680\n29     31    male  36.300         2    yes  southwest  38711.00000\n30     22    male  35.600         0    yes  southwest  35585.57600\n...   ...     ...     ...       ...    ...        ...          ...\n1300   45    male  30.360         0    yes  southeast  62592.87309\n1301   62    male  30.875         3    yes  northwest  46718.16325\n1303   43    male  27.800         0    yes  southwest  37829.72420\n1313   19  female  34.700         2    yes  southwest  36397.57600\n1323   42  female  40.370         2    yes  southeast  43896.37630\n\n[139 rows x 7 columns]\n\n\n\nprint(f\"Number of outliers using IQR method: {len(outliers_iqr)}\")\nprint(f\"Lower Bound: {lower_bound}\")\nprint(f\"Upper Bound: {upper_bound}\")\n\nNumber of outliers using IQR method: 139\nLower Bound: -13109.1508975\nUpper Bound: 34489.350562499996"
  },
  {
    "objectID": "5_outlier/index.html#data-preprocessing-filter-outlier-and-train-models",
    "href": "5_outlier/index.html#data-preprocessing-filter-outlier-and-train-models",
    "title": "Outlier detection",
    "section": "Data preprocessing, filter outlier, and train models",
    "text": "Data preprocessing, filter outlier, and train models\n\n##Converting objects labels into categorical\ndf[['sex', 'smoker', 'region']] = df[['sex', 'smoker', 'region']].astype('category')\ndf.dtypes\n\nage            int64\nsex         category\nbmi          float64\nchildren       int64\nsmoker      category\nregion      category\ncharges      float64\ndtype: object\n\n\n\n##Converting category labels into numerical using LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\nlabel.fit(df.sex.drop_duplicates())\ndf.sex = label.transform(df.sex)\nlabel.fit(df.smoker.drop_duplicates())\ndf.smoker = label.transform(df.smoker)\nlabel.fit(df.region.drop_duplicates())\ndf.region = label.transform(df.region)\ndf.dtypes\n\nage           int64\nsex           int64\nbmi         float64\nchildren      int64\nsmoker        int64\nregion        int64\ncharges     float64\ndtype: object\n\n\n\n# Filter outliers\ndata_filtered = df[(df['charges'] &gt;= lower_bound) & (df['charges'] &lt;= upper_bound)]\n# data_filtered = df\n\n# Split data into features and target variable\nX = data_filtered.drop(['charges'], axis=1)  # Features\ny = data_filtered['charges']  # Target variable\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and fit the polynomial regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict charges using the model\ny_pred = model.predict(X_test)\n\n# Evaluate the model using different metrics\nr2 = r2_score(y_test, y_pred)\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(f\"R^2 Score: {r2}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\nR^2 Score: 0.5580582759919098\nMean Absolute Error (MAE): 2791.8165704862845\nMean Squared Error (MSE): 27568455.24547175\nRoot Mean Squared Error (RMSE): 5250.567135602758"
  },
  {
    "objectID": "4_classification/index.html",
    "href": "4_classification/index.html",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "",
    "text": "dataset: https://www.kaggle.com/competitions/titanic/data\n\n\ntask: build a predictive model that predict whether a person survive in the titanic insident based on several factors (e.g., name, age, cabin, etc.)\nWe will learn some basic data exploration technique, feature engineering, and classification algorithm\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "4_classification/index.html#data-exploration",
    "href": "4_classification/index.html#data-exploration",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "Data Exploration",
    "text": "Data Exploration\nLets see the preview of the dataset\n\ndf_train = pd.read_csv('data/train.csv')\ndf_train.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ndf_test = pd.read_csv('data/test.csv')\ndf_test.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nNow let’s see some pattern on the dataset\n\nsns.barplot(x=df_train['Sex'], y=df_train['Survived']) \n\n&lt;Axes: xlabel='Sex', ylabel='Survived'&gt;\n\n\n\n\n\nBased on the plot above we can see that female are much more likely to survive compared to men\nNow let’s find the correlation among the features on the data using heatmap\n\nsns.heatmap(df_train.corr(),annot=True) \nfig=plt.gcf()\nplt.show()\n\n\n\n\nBased on the heatmap above we can see that “PClass” and “Fare” is highly correlated to the “Survived” column, whereas other column is not highly correlated with “Survived” column.  But since “Fare” also highly correlated with “PClass” then we just need “PClass” column"
  },
  {
    "objectID": "4_classification/index.html#feature-engineering",
    "href": "4_classification/index.html#feature-engineering",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nBased on our data exploration above we will only use column “Sex” and “PClass” to predict the survival of each person since those two features are highly correlated with the survival\nSo here is the step of feature engineering that we will do:\n\nConvert “Sex” column into a 0/1 valued column since this column has a categorical data (“female/male”). We can call this column “IsMale”\nCreate new column TicketFreq based on column Ticket. Will explain about this later\nDrop columns other unnecessary column\n\n\n## Create new column \"IsMale\"\n\ndf_train_transformed = df_train.copy()\ndf_train_transformed[\"IsMale\"] = df_train[\"Sex\"].apply(lambda x: x == \"male\")\ndf_train_transformed.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nIsMale\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\nTrue\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\nFalse\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nFalse\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\nFalse\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nTrue\n\n\n\n\n\n\n\nTicket Frequency is a ticket-based feature that includes people who have the same ticket number. This feature can serve as group size as it puts people who travel with the same ticket number together, whether they are related or not.\n\ndf_train_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n\n\n## Drop unwanted columns\ndf_train_transformed = df_train_transformed.drop(['PassengerId','Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \ndf_train_transformed.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nIsMale\nTicketFreq\n\n\n\n\n0\n0\n3\nTrue\n1\n\n\n1\n1\n1\nFalse\n1\n\n\n2\n1\n3\nFalse\n1\n\n\n3\n1\n1\nFalse\n2\n\n\n4\n0\n3\nTrue\n1\n\n\n\n\n\n\n\nSince we need to do the same feature engineering process on the test dataset, so let’s wrap all the process into a single function\n\ndef transform_data(df, is_train_dataset = True):\n    df_transformed = df.copy()\n    df_transformed[\"IsMale\"] = df[\"Sex\"].apply(lambda x: x == \"male\")\n    \n    df_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n    \n    df_transformed = df_transformed.drop(['Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \n    \n    if (is_train_dataset):\n        df_transformed = df_transformed.drop(['PassengerId'],axis=1)\n    \n    return df_transformed\n\n\ntemp_df = transform_data(df_train)\ntemp_df.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nIsMale\nTicketFreq\n\n\n\n\n0\n0\n3\nTrue\n1\n\n\n1\n1\n1\nFalse\n1\n\n\n2\n1\n3\nFalse\n1\n\n\n3\n1\n1\nFalse\n2\n\n\n4\n0\n3\nTrue\n1\n\n\n\n\n\n\n\n\nsns.barplot(x='IsMale', y='Survived', data=temp_df) \n\n&lt;Axes: xlabel='IsMale', ylabel='Survived'&gt;\n\n\n\n\n\n\nsns.barplot(x='TicketFreq', y='Survived', data=temp_df) \n\n&lt;Axes: xlabel='TicketFreq', ylabel='Survived'&gt;\n\n\n\n\n\n\nsns.barplot(x='Pclass', y='Survived', data=temp_df) \n\n&lt;Axes: xlabel='Pclass', ylabel='Survived'&gt;"
  },
  {
    "objectID": "4_classification/index.html#model-training",
    "href": "4_classification/index.html#model-training",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "Model Training",
    "text": "Model Training\nIn this part we will experiment with several classifier model\nTo measure performance we will split the training data into train and test. So we will not touch the real test dataset to measure performance during our training process, this is to prevent data test leak. We will use 80/20 train-test split\n\npredictors = ['IsMale', 'Pclass', 'TicketFreq']\nlabel = 'Survived'\n\n\ndf_train_transformed = transform_data(df_train)\n\nX = df_train_transformed[predictors]\ny = df_train_transformed[label]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n\nfrom sklearn import metrics\n\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n\nAccuracy: 0.8\nPrecision: 0.8080808080808081\nRecall: 0.6666666666666666\nF1: 0.730593607305936\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\n\n\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n\nAccuracy: 0.8\nPrecision: 0.8080808080808081\nRecall: 0.6666666666666666\nF1: 0.730593607305936\n\n\nBased on the comparison above, random forest classifier model gives better accuracy, that is why we will choose this model for our final prediction\n\ndf_test_transformed = transform_data(df_test)\nX_test_submission = df_test_transformed[predictors]\n\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X, y)\npredictions = model.predict(X_test_submission)\n\n\noutput= pd.DataFrame (pd.DataFrame({\n    \"PassengerId\": df_test[\"PassengerId\"],\n    \"Survived\": predictions}))\noutput.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n\n\n\n\n\n\noutput.to_csv('FinalSubmission.csv', index=False)\n\nHere is the result when we submit this to kaggle competition"
  }
]