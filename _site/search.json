[
  {
    "objectID": "website1/index.html",
    "href": "website1/index.html",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "",
    "text": "dataset: https://www.kaggle.com/competitions/titanic/data\n\n\ntask: build a predictive model that predict whether a person survive in the titanic insident based on several factors (e.g., name, age, cabin, etc.)\nWe will learn some basic data exploration technique, feature engineering, and classification algorithm\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "website1/index.html#data-exploration",
    "href": "website1/index.html#data-exploration",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "Data Exploration",
    "text": "Data Exploration\nLets see the preview of the dataset\n\ndf_train = pd.read_csv('data/train.csv')\ndf_train.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n\n\n\n\n\n\ndf_test = pd.read_csv('data/test.csv')\ndf_test.head()\n\n\n\n\n\n\n\n\nPassengerId\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n892\n3\nKelly, Mr. James\nmale\n34.5\n0\n0\n330911\n7.8292\nNaN\nQ\n\n\n1\n893\n3\nWilkes, Mrs. James (Ellen Needs)\nfemale\n47.0\n1\n0\n363272\n7.0000\nNaN\nS\n\n\n2\n894\n2\nMyles, Mr. Thomas Francis\nmale\n62.0\n0\n0\n240276\n9.6875\nNaN\nQ\n\n\n3\n895\n3\nWirz, Mr. Albert\nmale\n27.0\n0\n0\n315154\n8.6625\nNaN\nS\n\n\n4\n896\n3\nHirvonen, Mrs. Alexander (Helga E Lindqvist)\nfemale\n22.0\n1\n1\n3101298\n12.2875\nNaN\nS\n\n\n\n\n\n\n\nNow let’s see some pattern on the dataset\n\nsns.barplot(x=df_train['Sex'], y=df_train['Survived']) \n\n&lt;Axes: xlabel='Sex', ylabel='Survived'&gt;\n\n\n\n\n\nBased on the plot above we can see that female are much more likely to survive compared to men\nNow let’s find the correlation among the features on the data using heatmap\n\nsns.heatmap(df_train.corr(),annot=True) \nfig=plt.gcf()\nplt.show()\n\n\n\n\nBased on the heatmap above we can see that “PClass” and “Fare” is highly correlated to the “Survived” column, whereas other column is not highly correlated with “Survived” column.  But since “Fare” also highly correlated with “PClass” then we just need “PClass” column"
  },
  {
    "objectID": "website1/index.html#feature-engineering",
    "href": "website1/index.html#feature-engineering",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nBased on our data exploration above we will only use column “Sex” and “PClass” to predict the survival of each person since those two features are highly correlated with the survival\nSo here is the step of feature engineering that we will do:\n\nConvert “Sex” column into a 0/1 valued column since this column has a categorical data (“female/male”). We can call this column “IsMale”\nCreate new column TicketFreq based on column Ticket. Will explain about this later\nDrop columns other unnecessary column\n\n\n## Create new column \"IsMale\"\n\ndf_train_transformed = df_train.copy()\ndf_train_transformed[\"IsMale\"] = df_train[\"Sex\"].apply(lambda x: x == \"male\")\ndf_train_transformed.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\nIsMale\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\nTrue\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\nFalse\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\nFalse\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\nFalse\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\nTrue\n\n\n\n\n\n\n\nTicket Frequency is a ticket-based feature that includes people who have the same ticket number. This feature can serve as group size as it puts people who travel with the same ticket number together, whether they are related or not.\n\ndf_train_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n\n\n## Drop unwanted columns\ndf_train_transformed = df_train_transformed.drop(['PassengerId','Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \ndf_train_transformed.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nIsMale\nTicketFreq\n\n\n\n\n0\n0\n3\nTrue\n1\n\n\n1\n1\n1\nFalse\n1\n\n\n2\n1\n3\nFalse\n1\n\n\n3\n1\n1\nFalse\n2\n\n\n4\n0\n3\nTrue\n1\n\n\n\n\n\n\n\nSince we need to do the same feature engineering process on the test dataset, so let’s wrap all the process into a single function\n\ndef transform_data(df, is_train_dataset = True):\n    df_transformed = df.copy()\n    df_transformed[\"IsMale\"] = df[\"Sex\"].apply(lambda x: x == \"male\")\n    \n    df_transformed['TicketFreq'] = df_train.groupby('Ticket')['Ticket'].transform('count')\n    \n    df_transformed = df_transformed.drop(['Ticket', 'Sex', 'Name', 'Fare', 'Age', 'Parch', 'SibSp','Cabin', 'Embarked'], axis=1) \n    \n    if (is_train_dataset):\n        df_transformed = df_transformed.drop(['PassengerId'],axis=1)\n    \n    return df_transformed\n\n\ntemp_df = transform_data(df_train)\ntemp_df.head()\n\n\n\n\n\n\n\n\nSurvived\nPclass\nIsMale\nTicketFreq\n\n\n\n\n0\n0\n3\nTrue\n1\n\n\n1\n1\n1\nFalse\n1\n\n\n2\n1\n3\nFalse\n1\n\n\n3\n1\n1\nFalse\n2\n\n\n4\n0\n3\nTrue\n1\n\n\n\n\n\n\n\n\nsns.barplot(x='IsMale', y='Survived', data=temp_df) \n\n&lt;Axes: xlabel='IsMale', ylabel='Survived'&gt;\n\n\n\n\n\n\nsns.barplot(x='TicketFreq', y='Survived', data=temp_df) \n\n&lt;Axes: xlabel='TicketFreq', ylabel='Survived'&gt;\n\n\n\n\n\n\nsns.barplot(x='Pclass', y='Survived', data=temp_df) \n\n&lt;Axes: xlabel='Pclass', ylabel='Survived'&gt;"
  },
  {
    "objectID": "website1/index.html#model-training",
    "href": "website1/index.html#model-training",
    "title": "Learning Classification Algorithm using Titanic Dataset",
    "section": "Model Training",
    "text": "Model Training\nIn this part we will experiment with several classifier model\nTo measure performance we will split the training data into train and test. So we will not touch the real test dataset to measure performance during our training process, this is to prevent data test leak. We will use 80/20 train-test split\n\npredictors = ['IsMale', 'Pclass', 'TicketFreq']\nlabel = 'Survived'\n\n\ndf_train_transformed = transform_data(df_train)\n\nX = df_train_transformed[predictors]\ny = df_train_transformed[label]\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n\nfrom sklearn import metrics\n\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n\nAccuracy: 0.8067796610169492\nPrecision: 0.8118811881188119\nRecall: 0.6833333333333333\nF1: 0.7420814479638009\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\npredictions = dt.predict(X_test)\n\n\nprint('Accuracy:', metrics.accuracy_score(y_test, predictions))\nprint('Precision:', metrics.precision_score(y_test,predictions))\nprint('Recall:', metrics.recall_score(y_test, predictions))\nprint('F1:', metrics.f1_score(y_test, predictions))\n\nAccuracy: 0.8\nPrecision: 0.8080808080808081\nRecall: 0.6666666666666666\nF1: 0.730593607305936\n\n\nBased on the comparison above, random forest classifier model gives better accuracy, that is why we will choose this model for our final prediction\n\ndf_test_transformed = transform_data(df_test)\nX_test_submission = df_test_transformed[predictors]\n\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X, y)\npredictions = model.predict(X_test_submission)\n\n\noutput= pd.DataFrame (pd.DataFrame({\n    \"PassengerId\": df_test[\"PassengerId\"],\n    \"Survived\": predictions}))\noutput.head()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\n\n\n\n\n0\n892\n0\n\n\n1\n893\n1\n\n\n2\n894\n0\n\n\n3\n895\n0\n\n\n4\n896\n1\n\n\n\n\n\n\n\n\noutput.to_csv('FinalSubmission.csv', index=False)\n\nHere is the result when we submit this to kaggle competition"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "Author: Taufiq Husada Daryanto\nCourse: Machine Learning CS 5805\n\n\nList of blogs:\n\n\nClassification: End-to-end machine learning projects using classification algorithm on titanic dataset"
  }
]